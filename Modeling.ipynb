{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modeling.ipynb","provenance":[],"authorship_tag":"ABX9TyOs6cl0ROnunbvXArjRnord"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TL7Hckx6LNGa","colab_type":"text"},"source":["Replicate validation score"]},{"cell_type":"code","metadata":{"id":"_zxGUpN6LETt","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","\n","# Calculate the mean fare_amount on the validation_train data\n","naive_prediction = np.mean(validation_train['fare_amount'])\n","\n","# Assign naive prediction to all the holdout observations\n","validation_test['pred'] = naive_prediction\n","\n","# Measure the local RMSE\n","rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n","print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5z6gm7KLUxx","colab_type":"text"},"source":["Baseline based on the data"]},{"cell_type":"code","metadata":{"id":"LsX9GFDsLZpP","colab_type":"code","colab":{}},"source":["# Get pickup hour from the pickup_datetime column\n","train['hour'] = train['pickup_datetime'].dt.hour\n","test['hour'] = test['pickup_datetime'].dt.hour\n","\n","# Calculate average fare_amount grouped by pickup hour \n","hour_groups = train.groupby('hour')['fare_amount'].mean()\n","\n","# Make predictions on the test set\n","test['fare_amount'] = test.hour.map(hour_groups)\n","\n","# Write predictions\n","test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ56KBf-LeS-","colab_type":"text"},"source":["Baseline Based on Gradient Boosting"]},{"cell_type":"code","metadata":{"id":"ApztmMx2LfYj","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Select only numeric features\n","features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n","            'dropoff_latitude', 'passenger_count', 'hour']\n","\n","# Train a Random Forest model\n","rf = RandomForestRegressor()\n","rf.fit(train[features], train.fare_amount)\n","\n","# Make predictions on the test data\n","test['fare_amount'] = rf.predict(test[features])\n","\n","# Write predictions\n","test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsj3CQXMLoN3","colab_type":"text"},"source":["Grid Search"]},{"cell_type":"code","metadata":{"id":"kMXuNFT2LpXa","colab_type":"code","colab":{}},"source":["# Possible max depth values\n","max_depth_grid = [3,6,9,12,15]\n","results = {}\n","\n","# For each value in the grid\n","for max_depth_candidate in max_depth_grid:\n","    # Specify parameters for the model\n","    params = {'max_depth': max_depth_candidate}\n","\n","    # Calculate validation score for a particular hyperparameter\n","    validation_score = get_cv_score(train, params)\n","\n","    # Save the results for each max depth value\n","    results[max_depth_candidate] = validation_score   \n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NPc66m-L3yu","colab_type":"text"},"source":["2D grid search"]},{"cell_type":"code","metadata":{"id":"ZtefiBWgL4o1","colab_type":"code","colab":{}},"source":["import itertools\n","\n","# Hyperparameter grids\n","max_depth_grid = [3,5,7]\n","subsample_grid = [.8,0.9,1.0]\n","results = {}\n","\n","# For each couple in the grid\n","for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n","    params = {'max_depth': max_depth_candidate,\n","              'subsample': subsample_candidate}\n","    validation_score = get_cv_score(train, params)\n","    # Save the results for each couple\n","    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTaeVyeDMZw4","colab_type":"text"},"source":["ModelBlending"]},{"cell_type":"code","metadata":{"id":"sZ0CqWaSMbhb","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","\n","# Train a Gradient Boosting model\n","gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n","\n","# Train a Random Forest model\n","rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n","\n","# Make predictions on the test data\n","test['gb_pred'] = gb.predict(test[features])\n","test['rf_pred'] = rf.predict(test[features])\n","\n","# Find mean of model predictions\n","test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n","print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXqNslQfMfXK","colab_type":"text"},"source":["Model Stacking I"]},{"cell_type":"code","metadata":{"id":"tpbhNSZwMhFR","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","\n","# Split train data into two parts\n","part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n","\n","# Train a Gradient Boosting model on Part 1\n","gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n","\n","# Train a Random Forest model on Part 1\n","rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mksTvZFpMkVk","colab_type":"code","colab":{}},"source":["# Make predictions on the Part 2 data\n","part_2['gb_pred'] = gb.predict(part_2[features])\n","part_2['rf_pred'] = rf.predict(part_2[features])\n","\n","# Make predictions on the test data\n","test['gb_pred'] = gb.predict(test[features])\n","test['rf_pred'] = rf.predict(test[features])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKCeFUwdMlnG","colab_type":"text"},"source":["Model Stacking II"]},{"cell_type":"code","metadata":{"id":"WDeXBhzmMqKW","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","\n","# Create linear regression model without the intercept\n","lr = LinearRegression(fit_intercept=False)\n","\n","# Train 2nd level model on the Part 2 data\n","lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n","\n","# Make stacking predictions on the test data\n","test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n","\n","# Look at the model coefficients\n","print(lr.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xvy9o_dyMwmN","colab_type":"text"},"source":["Testing Kaggle Fourm"]},{"cell_type":"code","metadata":{"id":"LoUfHP1EM0XI","colab_type":"code","colab":{}},"source":["# Drop passenger_count column\n","new_train_1 = train.drop('passenger_count', axis=1)\n","\n","# Compare validation scores\n","initial_score = get_cv_score(train)\n","new_score = get_cv_score(new_train_1)\n","\n","print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"],"execution_count":null,"outputs":[]}]}